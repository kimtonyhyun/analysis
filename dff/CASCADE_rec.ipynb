{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HelmchenLabSoftware/Cascade/blob/master/Demo%20scripts/Calibrated_spike_inference_with_Cascade.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECkMdA5pMRSY"
   },
   "source": [
    "# CASCADE\n",
    "\n",
    "## Calibrated spike inference from calcium imaging data using deep networks\n",
    "Written and maintained by [Peter Rupprecht](https://github.com/PTRRupprecht) and [Adrian Hoffmann](https://github.com/AdrianHoffmann) from the [Helmchen Lab](https://www.hifo.uzh.ch/en/research/helmchen.html).\n",
    "The project started as a collaboration of the Helmchen Lab and the [Friedrich Lab](https://www.fmi.ch/research-groups/groupleader.html?group=119). Feedback goes to [Peter Rupprecht](mailto:p.t.r.rupprecht+cascade@gmail.com).\n",
    "\n",
    "---\n",
    "\n",
    "Adapted by Tony Hyun Kim, Sept 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yfupSpg7FtpN",
    "outputId": "a9be9eeb-2c0c-496e-873c-ed5356fa164d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\herrin\\Documents\\Cascade\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path_to_cascade = 'C:/Users/herrin/Documents/Cascade'\n",
    "os.chdir(path_to_cascade)\n",
    "print('Current working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "u-cs8Jze8MpX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ruamel.yaml in c:\\users\\herrin\\anaconda3\\envs\\cascade\\lib\\site-packages (0.16.12)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.1.2 in c:\\users\\herrin\\anaconda3\\envs\\cascade\\lib\\site-packages (from ruamel.yaml) (0.2.2)\n",
      "\tYAML reader installed (version 0.16.12).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tKeras installed (version 2.3.1).\n",
      "\tTensorflow installed (version 2.1.0).\n"
     ]
    }
   ],
   "source": [
    "#@markdown Downloads packages from public repository, and packages from Cascade.\n",
    "\n",
    "!pip install ruamel.yaml\n",
    "\n",
    "# standard python packages\n",
    "import os, warnings\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import ruamel.yaml as yaml\n",
    "import h5py\n",
    "\n",
    "# cascade2p packages, imported from the downloaded Github repository\n",
    "from cascade2p import checks\n",
    "checks.check_packages()\n",
    "from cascade2p import cascade # local folder\n",
    "from cascade2p.utils import plot_dFF_traces, plot_noise_level_distribution, plot_noise_matched_ground_truth\n",
    "\n",
    "# THK: Loads traces from REC files\n",
    "def load_traces(file_path):\n",
    "    \"\"\"Load traces from REC files with output shape [num_neuron x num_frames]\"\"\"\n",
    "    f = h5py.File(file_path, 'r')\n",
    "    \n",
    "    traces = np.array(f['traces'])\n",
    "    info = f['info']\n",
    "    frame_rate = info['fps'][()].item()\n",
    "    \n",
    "    return traces, frame_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Located REC file: Y:/Downloads/oh12-0123-ctx/union_15hz/dff\\rec_210905-151523.mat\n"
     ]
    }
   ],
   "source": [
    "path_to_rec = \"Y:/Downloads/oh12-0123-ctx/union_15hz/dff\"\n",
    "\n",
    "path_to_rec = os.path.join(path_to_rec, \"rec_*.mat\")\n",
    "rec_file = glob.glob(path_to_rec)[0]\n",
    "print('Located REC file:', rec_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wKyE8JVV8fMg",
    "outputId": "fb08a569-86a4-4a44-f52e-e8e8bd190df3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y:/Downloads/oh12-0123-ctx/union_15hz/dff\\rec_210905-151523.mat\n",
      "Number of neurons in dataset: 39\n",
      "Number of timepoints in dataset: 16000\n",
      "Frame rate: 15.0\n"
     ]
    }
   ],
   "source": [
    "#@markdown If you are testing the script, you can leave everything unchanged. If you want to apply the algorithm to your own data, you have to upload your data first. The paragraph above tells you how to format and name the file. You can do this by clicking on the **folder symbol (\"Files\")** on the left side of the Colaboratory notebook. Next, indicate the path of the uploaded file in the variable **`example_file`**. Finally, indicate the sampling rate of your recordings in the variable **`frame_rate`**.\n",
    "\n",
    "try:\n",
    "\n",
    "  traces, frame_rate = load_traces(rec_file)\n",
    "  print(rec_file)\n",
    "  print('Number of neurons in dataset:', traces.shape[0])\n",
    "  print('Number of timepoints in dataset:', traces.shape[1])\n",
    "  print('Frame rate:', frame_rate)\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "  print('Error message: '+str(e))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n8uu8p_XETZ5",
    "outputId": "3d3f223a-054f-476c-8e2d-88ab897199bc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      "The selected model was trained on 18 datasets, with 5 ensembles for each noise level, at a sampling rate of 15Hz, with a resampled ground truth that was smoothed with a Gaussian kernel of a standard deviation of 200 milliseconds. \n",
      " \n",
      "\n",
      "Loaded model was trained at frame rate 15 Hz\n",
      "Given argument traces contains 39 neurons and 16000 frames.\n",
      "Noise levels (mean, std; in standard units): 1.37, 0.19\n",
      "\n",
      "Predictions for noise level 2:\n",
      "\t... ensemble 0\n",
      "624000/624000 [==============================] - 7s 11us/sample\n",
      "\t... ensemble 1\n",
      "624000/624000 [==============================] - 7s 12us/sample\n",
      "\t... ensemble 2\n",
      "624000/624000 [==============================] - 7s 11us/sample\n",
      "\t... ensemble 3\n",
      "624000/624000 [==============================] - 7s 12us/sample\n",
      "\t... ensemble 4\n",
      "624000/624000 [==============================] - 7s 11us/sample\n",
      "\n",
      "Predictions for noise level 3:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 4:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 5:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 6:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 7:\n",
      "\tNo neurons for this noise level\n",
      "\n",
      "Predictions for noise level 8:\n",
      "\tNo neurons for this noise level\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Global_EXC_15Hz_smoothing200ms\" #@param {type:\"string\"}\n",
    "\n",
    "spike_probs = cascade.predict( model_name, traces )\n",
    "\n",
    "# save as mat file\n",
    "folder = os.path.dirname(rec_file)\n",
    "file_name = 'cascade_' + model_name\n",
    "save_path = os.path.join(folder, file_name)\n",
    "\n",
    "sio.savemat(save_path+'.mat', {'spike_probs': np.transpose(spike_probs), 'model_name': model_name, 'rec_file': rec_file})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Calibrated spike inference with Cascade.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Cascade",
   "language": "python",
   "name": "cascade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
